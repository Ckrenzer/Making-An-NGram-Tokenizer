---
title: "Making An N-Gram Tokenizer"
author: "Connor Krenzer"
date: "8/5/2021"
output: html_document
---


### Background

I am a big fan of [Julia Silge's](https://juliasilge.com/) work. In the second chapter of the book she co-authored with [Emil Hvitfeldt](https://www.emilhvitfeldt.com/), [Supervised Machine Learning For Text Analysis in R](https://smltar.com/index.html), Julia goes in great detail on how to create a tokenizer for text datasets. She explains how tokenizers work, the difficulties of tokenizing different languages, and the different types of tokens. Importantly, she shows us how to create our own tokenizer for our own use cases and in doing so touches upon one of the most important heuristics of the R language: **functions gain flexibility at the expense of execution speed.** This is not always the case, but it holds in most cases. In Chapter 24 of Hadley's Advanced R, [Improving Performance](https://adv-r.hadley.nz/perf-improve.html#be-lazy), you see echoes of this claim. The more error-checking and customizable features that your function has, the more time your functions spend ensuring customizability rather than computing the desired value.





```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = FALSE,
                      include = TRUE,
                      results = "show",
                      error = FALSE,
                      warning = FALSE,
                      message = FALSE)

```






```{r packages, echo = TRUE}

# PACKAGES
if(!require(pacman)) install.packages("pacman")


# General Packages
pacman::p_load(stringr)


# Packages with n-gram tokenizers
pacman::p_load(tokenizers, tidytext, corpus, text2vec, quanteda)


# Benchmarking packages
pacman::p_load(bench)

```



