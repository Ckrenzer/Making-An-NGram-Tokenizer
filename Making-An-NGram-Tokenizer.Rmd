---
title: "Making An N-Gram Tokenizer"
author: "Connor Krenzer"
date: "8/5/2021"
output: html_document
---

```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = FALSE,
                      include = TRUE,
                      results = "show",
                      error = FALSE,
                      warning = FALSE,
                      message = FALSE)

```



#### Background

I am a big fan of [Julia Silge's](https://juliasilge.com/) work. Her writings and tutorials not only show you how to be an advanced user of algorithms, but she also provides references for those looking to further their understandings of them. In the second chapter of the book she co-authored with [Emil Hvitfeldt](https://www.emilhvitfeldt.com/), [Supervised Machine Learning For Text Analysis in R](https://smltar.com/index.html), Julia writes in great detail about how to create a tokenizer for text datasets. She explains how tokenizers work, the difficulties of tokenizing different human languages, and importantly, she shows us how to create our own tokenizers for our own use cases. In doing so, she touches upon one of the most important heuristics of the R language and indeed programming more broadly: **functions gain flexibility at the expense of execution speed.** This is not always the case, but it holds much of the time. In Chapter 24 of Hadley's Advanced R, [Improving Performance](https://adv-r.hadley.nz/perf-improve.html#be-lazy), you see echoes of this claim. The more error-checking and customizable features that your function has, the more time your functions spend ensuring customizability rather than computing the return value.

The goal, of course, is to have both--we want speed and customizability. Julia does a great job explaining the trade-offs inherent in the different tokenizer options out there, and she even provided us with some code to build our own tokenizers, but those of us tired of using black boxes want the ability to create our own functions. Julia provided us with a 'hand-written' function to tokenize words and letters, but she did not provide us with a tokenizer for n-grams. After reading her chapter, I wanted to build an n-gram tokenizer from the ground up, and I wanted it to be fast. This guide examines some clever ways to assemble n-grams that are viable competitors to the popular ones found on CRAN.

We will start with the thought-process followed and potential bottlenecks associated with the approach, then we will implement different tokenizers, and close by benchmarking our functions on a text dataset.







#### The Process

Any value of n in n-grams can be constructed from unigrams. In practical terms, this means placing each word of a document into its own element of a vector and then pasting *n* adjacent words together to make the n-gram. For example, with n = 2 on the sentence "My short and exquisite sentence", we should have {"My short", "short and", "and exquisite", "exquisite sentence"} after tokenizing. Think of the tokenizer as a slide that selects two (n) words at a time until each pair of words is selected.

One of the greatest bottlenecks in this approach is the step creating unigrams. If this step can be bypassed (perhaps with a well-designed regular expression), the algorithms can speed up further.






#### Packages

Let's load in all the packages we'll be using. Many of these are only used to compare the tokenizers written here to the tokenizers from official packages.

```{r packages, echo = TRUE}

if(!require(pacman)) install.packages("pacman")
# General Packages
pacman::p_load(stringr, data.table)

# Packages with n-gram tokenizers
pacman::p_load(tokenizers, tidytext, corpus, text2vec, quanteda)

# Benchmarking packages
pacman::p_load(bench)

```









#### Tokenizers

As mentioned above, the more restricted a function's use, the faster the function's execution speed. The more complex the data structure used in the tokenization, the slower the function's speed. Often, we will want the tokenizer to return a data frame instead of a character vector. This will cost us dearly, but the flexibility gained from doing so is often worth the cost.

Let's begin with a simple tokenizer and work our way up in complexity.



##### Heads and Tails

Often, we only need bigrams (n = 2). After splitting the text into unigrams, `head()` and `tail()` combine the words into bigrams quite nicely:

```{r f1}

f1 <- function(words){
  # the unigrams
  words <- str_split(words, "\\s+")[[1]]
  
  return(str_c(head(words, -1), tail(words, -1), sep = " ", collapse = NULL))
}

```

-   Split the string into unigrams.

-   Paste the vectors together using `head()` and `tail()`.

-   **Bigrams only**.




##### Nested Loops

For those of us who like to think about problems by tracing out each step, loops work wonders. It is not guaranteed to be the fastest implementation in R, but this methodology translates nicely to other languages:

```{r f2}

f2 <- function(words, n = 2){
  # making unigrams
  words <- str_split(words, "\\s+")[[1]]
  n <- n - 1
  
  vec <- character(length(words) - n)
  for(i in 1:length(vec)){
    for(j in i:(i + n)){
      vec[i] <- str_c(vec[i], words[j], sep = " ") 
    }
  }
  vec <- str_remove(vec, "^\\s{1}")
  return(vec)
}

```

-   Split the string into unigrams.

-   Use nested loops to assemble pairings of *n* words together.

-   **Works for any value of n**.



##### Shift Lists

Data.table's `shift()` function provides us with a bona fide slider as described above. We can then use loop functions to assemble the n-grams. We all keep a shift list:

```{r f3}

# Does not work for n = 1
f3 <- function(words, n = 2){
  # the unigrams
  words <- str_split(words, "\\s+")[[1]]
  n <- n - 1
  
  word_list <- lapply(shift(words, n = 0:n, type = 'lead'), na.omit)
  mn <- min(lengths(word_list))
  grams <- do.call(paste, lapply(word_list, head, mn))
  
  return(grams)
}

```

-   Use `data.table::shift()` to put words into *n* lists.

-   Use the length of the shortest list when calling the `head()` function and assemble the word vector with `paste()` and `do.call()`.

-   **Does not work for n = 1**.






##### Tibbles

Of course, we might want to use this in a magrittr pipeline. When this is the case, character vectors just won't cut it.

```{r f1_tibble}

f1_tibble <- function(words){
  # the unigrams
  words <- str_split(words, "\\s+")[[1]]
  
  return(tibble(text = str_c(head(words, -1), tail(words, -1), sep = " ", collapse = NULL)))
}

```



```{r f2_tibble}

f2_tibble <- function(words, n = 2){
  # making unigrams
  words <- str_split(words, "\\s+")[[1]]
  n <- n - 1
  
  vec <- character(length(words) - n)
  for(i in 1:length(vec)){
    for(j in i:(i + n)){
      vec[i] <- str_c(vec[i], words[j], sep = " ") 
    }
  }
  vec <- str_remove(vec, "^\\s{1}")
  return(tibble(text = vec))
}

```



```{r f3_tibble}

f3_tibble <- function(words, n = 2){
  # the unigrams
  words <- str_split(words, "\\s+")[[1]]
  n <- n - 1
  
  word_list <- lapply(shift(words, n = 0:n, type = 'lead'), na.omit)
  mn <- min(lengths(word_list))
  grams <- do.call(paste, lapply(word_list, head, mn))
  
  return(tibble(text = grams))
}

```



```{r f2_tibble_customizable}

f2_tibble_customizable <- function(text_df, key_column = "company", text_column = "value", n = 2){
  # making unigrams
  words <- str_split(text_df[[text_column]], "\\s+", simplify = FALSE)
  n <- n - 1
  
  # results will be added to this data frame
  ngram_df <- tibble(company = character(0), text = character(0))
  
  # performing this operation for each 'key' in the data frame
  for(element in 1:length(words)){
    # the company name is our key
    key_name <- text_df[[key_column]][element]
    
    # the n-grams are added to this vector
    vec <- character(length(words[[element]]) - n)
    for(i in 1:length(vec)){
      for(j in i:(i + n)){
        vec[i] <- str_c(vec[i], words[[element]][j], sep = " ") 
      }
    }
    vec <- tibble(company = key_name, text = str_remove(vec, "^\\s{1}"))
    ngram_df <- bind_rows(ngram_df, vec) 
  }
  
  return(ngram_df)
}

```





#### Evaluation

If we are going to build an n-grams tokenizer, we need a text dataset! Let's use my favorite novel, [The Count of Monte Cristo](https://www.gutenberg.org/files/1184/1184-0.txt). The version used is the one from the `corpus` package's Project Gutenberg API, except the footnotes at the end of the book are removed. The data is stored in a text file to ensure reproducibility.

The Count is the closest thing the 19th century world had to the Most Interesting Man In The World; if you have time to spare, give it a read!

```{r data}

# The Count of Monte Cristo by Alexandre Dumas (English Translation)
cristo <- readLines("The Count of Monte Cristo.txt")

# We want the data represented in a single string for tokenization
cristo <- str_c(cristo, collapse = " ")

```
