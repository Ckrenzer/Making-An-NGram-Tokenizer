---
title: "Making An N-Gram Tokenizer"
author: "Connor Krenzer"
date: "8/5/2021"
output: html_document
---


#### Background

I am a big fan of [Julia Silge's](https://juliasilge.com/) work. In the second chapter of the book she co-authored with [Emil Hvitfeldt](https://www.emilhvitfeldt.com/), [Supervised Machine Learning For Text Analysis in R](https://smltar.com/index.html), Julia writes in great detail about how to create a tokenizer for text datasets. She explains how tokenizers work, the difficulties of tokenizing different human languages, and importantly, she shows us how to create our own tokenizer for our own use cases. In doing so, she touches upon one of the most important heuristics of the R language and indeed programming more broadly: **functions gain flexibility at the expense of execution speed.** This is not always the case, but it holds much of the time. In Chapter 24 of Hadley's Advanced R, [Improving Performance](https://adv-r.hadley.nz/perf-improve.html#be-lazy), you see echoes of this claim. The more error-checking and customizable features that your function has, the more time your functions spend ensuring customizability rather than computing the return value.

The goal, of course, is to have both--we want speed and customizability. Julia does a great job explaining the trade-offs inherent in the different tokenizer options out there, and she even provided us with some code to build our own tokenizers, but those of us tired of using black boxes want the ability to create our own functions. After reading her chapter, I wanted to build a tokenizer form the ground up.



```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = FALSE,
                      include = TRUE,
                      results = "show",
                      error = FALSE,
                      warning = FALSE,
                      message = FALSE)

```






```{r packages, echo = TRUE}

# PACKAGES
if(!require(pacman)) install.packages("pacman")


# General Packages
pacman::p_load(stringr)


# Packages with n-gram tokenizers
pacman::p_load(tokenizers, tidytext, corpus, text2vec, quanteda)


# Benchmarking packages
pacman::p_load(bench)

```



